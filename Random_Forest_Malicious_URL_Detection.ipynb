{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9819264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "['br-icloud.com.br', 'phishing']\n",
      "['mp3raid.com/music/krizz_kaliko.html', 'benign']\n",
      "['bopsecrets.org/rexroth/cr/1.htm', 'benign']\n",
      "['http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15', 'defacement']\n",
      "['http://adventure-nicaragua.net/index.php?option=com_mailto&tmpl=component&link=aHR0cDovL2FkdmVudHVyZS1uaWNhcmFndWEubmV0L2luZGV4LnBocD9vcHRpb249Y29tX2NvbnRlbnQmdmlldz1hcnRpY2xlJmlkPTQ3OmFib3V0JmNhdGlkPTM2OmRlbW8tYXJ0aWNsZXMmSXRlbWlkPTU0', 'defacement']\n",
      "\n",
      "Type counts:\n",
      "phishing: 93818 URLs\n",
      "benign: 427883 URLs\n",
      "defacement: 96062 URLs\n",
      "malware: 32520 URLs\n",
      "s/: 1 URLs\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier from Scratch for Malicious URL Detection\n",
    "# No external libraries used â€” aligns with improving ML transparency in security\n",
    "\n",
    "# ---------------------------\n",
    "# Section 1: Dataset Loading and Preprocessing\n",
    "# ---------------------------\n",
    "\n",
    "def read_csv_manual(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    header = lines[0].strip().split(',')\n",
    "    data = [line.strip().split(',') for line in lines[1:] if len(line.strip().split(',')) == len(header)]\n",
    "    return header, data\n",
    "\n",
    "header, data = read_csv_manual(\"malicious_phish.csv\")\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "for row in data[:5]:\n",
    "    print(row)\n",
    "\n",
    "# Extract URLs and labels\n",
    "url_index = header.index(\"url\")\n",
    "type_index = header.index(\"type\")\n",
    "\n",
    "# Count of each type\n",
    "print(\"\\nType counts:\")\n",
    "type_counts = {}\n",
    "for row in data:\n",
    "    if len(row) > type_index:\n",
    "        label = row[type_index]\n",
    "        type_counts[label] = type_counts.get(label, 0) + 1\n",
    "for k, v in type_counts.items():\n",
    "    print(f\"{k}: {v} URLs\")\n",
    "\n",
    "urls = [row[url_index] for row in data]\n",
    "labels = [0 if row[type_index] == 'benign' else 1 for row in data if len(row) > type_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b28b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# WHOIS-based domain feature: domain expiration in days\n",
    "# ---------------------------\n",
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def domain_expiration_days(url):\n",
    "    try:\n",
    "        domain = re.findall(r\"https?://([^/]+)\", url)[0]\n",
    "        info = whois.whois(domain)\n",
    "        expiration_date = info.expiration_date\n",
    "\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "        if expiration_date is None:\n",
    "            return 0  # suspicious\n",
    "        delta = (expiration_date - datetime.utcnow()).days\n",
    "        return max(delta, 0)\n",
    "    except Exception:\n",
    "        return 0  # fallback for failed lookups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cad8f4-6b76-44f1-ab9b-f6e5f69e0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Section 2: Feature Extraction\n",
    "# ---------------------------\n",
    "\n",
    "def extract_features(url):\n",
    "    def count_digits(s):\n",
    "        return sum(1 for c in s if '0' <= c <= '9')\n",
    "\n",
    "    def starts_with_https(s):\n",
    "        return 1 if s.startswith(\"https\") else 0\n",
    "\n",
    "    def has_ip(s):\n",
    "        import re\n",
    "        return 1 if re.match(r\"http[s]?://(?:\\d{1,3}\\.){3}\\d{1,3}\", s) else 0\n",
    "\n",
    "    return [\n",
    "        domain_expiration_days(url),  # WHOIS feature\n",
    "        len(url),\n",
    "        url.count('.'),\n",
    "        url.count('-'),\n",
    "        url.count('_'),\n",
    "        url.count('/'),\n",
    "        url.count('?'),\n",
    "        url.count('='),\n",
    "        url.count('@'),\n",
    "        count_digits(url),\n",
    "        has_ip(url),\n",
    "        starts_with_https(url)\n",
    "    ]\n",
    "\n",
    "X = [extract_features(url) for url in urls]\n",
    "# Ensure y is properly defined alongside X\n",
    "y = labels\n",
    "\n",
    "# Normalize features manually\n",
    "# Global normalization stats\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "def compute_normalization_stats(X):\n",
    "    global means, stds\n",
    "    n = len(X[0])\n",
    "    means = [sum(row[i] for row in X) / len(X) for i in range(n)]\n",
    "    stds = [\n",
    "        (sum((row[i] - means[i]) ** 2 for row in X) / len(X)) ** 0.5\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "def normalize_features(X):\n",
    "    global means, stds\n",
    "    n = len(X[0])\n",
    "    return [\n",
    "        [(row[i] - means[i]) / stds[i] if stds[i] != 0 else 0 for i in range(n)]\n",
    "        for row in X\n",
    "    ]\n",
    "\n",
    "compute_normalization_stats(X)\n",
    "X = normalize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174ec511-8ddf-42c0-922e-87a0ebe20ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full dataset for training/testing\n",
    "import time\n",
    "\n",
    "def train_test_split_manual(X, y, test_ratio=0.3):\n",
    "    combined = list(zip(X, y))\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(combined)\n",
    "    split_idx = int(len(combined) * (1 - test_ratio))\n",
    "    train = combined[:split_idx]\n",
    "    test = combined[split_idx:]\n",
    "    X_train, y_train = zip(*train)\n",
    "    X_test, y_test = zip(*test)\n",
    "    return list(X_train), list(X_test), list(y_train), list(y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_manual(X, y, test_ratio=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a36612-476e-4397-87a5-b62dc1f240ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Section 4: Random Forest Core Code\n",
    "# ---------------------------\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def gini_index(groups):\n",
    "    total = sum(len(group) for group in groups)\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        labels = [row[-1] for row in group]\n",
    "        for class_val in [0, 1]:\n",
    "            proportion = labels.count(class_val) / size\n",
    "            score += proportion * proportion\n",
    "        gini += (1 - score) * (size / total)\n",
    "    return gini\n",
    "\n",
    "def split_data(index, threshold, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[0][index] < threshold:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "def best_split(dataset):\n",
    "    best_idx, best_thresh, best_score = None, None, float('inf')\n",
    "    best_groups = None\n",
    "    n_features = len(dataset[0][0])\n",
    "    for i in range(n_features):\n",
    "        thresholds = set(row[0][i] for row in dataset)\n",
    "        for t in thresholds:\n",
    "            groups = split_data(i, t, dataset)\n",
    "            gini = gini_index(groups)\n",
    "            if gini < best_score:\n",
    "                best_idx, best_thresh, best_score, best_groups = i, t, gini, groups\n",
    "    return best_idx, best_thresh, best_groups\n",
    "\n",
    "def build_tree(dataset, max_depth, min_size, depth=0):\n",
    "    labels = [row[1] for row in dataset]\n",
    "    if labels.count(labels[0]) == len(labels):\n",
    "        return DecisionTreeNode(value=labels[0])\n",
    "    if depth >= max_depth or len(dataset) <= min_size:\n",
    "        return DecisionTreeNode(value=max(set(labels), key=labels.count))\n",
    "    index, threshold, (left, right) = best_split(dataset)\n",
    "    if not left or not right:\n",
    "        return DecisionTreeNode(value=max(set(labels), key=labels.count))\n",
    "    left_node = build_tree(left, max_depth, min_size, depth + 1)\n",
    "    right_node = build_tree(right, max_depth, min_size, depth + 1)\n",
    "    return DecisionTreeNode(index, threshold, left_node, right_node)\n",
    "\n",
    "def predict_tree(node, row):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if row[node.feature_index] < node.threshold:\n",
    "        return predict_tree(node.left, row)\n",
    "    else:\n",
    "        return predict_tree(node.right, row)\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=5, max_depth=10, min_size=10):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.trees = []\n",
    "\n",
    "    def subsample(self, data):\n",
    "        import random\n",
    "        return [random.choice(data) for _ in range(len(data))]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = list(zip(X, y))\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            sample = self.subsample(dataset)\n",
    "            tree = build_tree(sample, self.max_depth, self.min_size)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, row):\n",
    "        predictions = [predict_tree(tree, row) for tree in self.trees]\n",
    "        return max(set(predictions), key=predictions.count)\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        return [self.predict(row) for row in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fcd816-4e7f-4d15-94d4-98de9b70dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88210840347334\n",
      "Precision: 0.8338093129094825\n",
      "Recall: 0.8183739544923104\n",
      "F1 Score: 0.8260195320478392\n",
      "Training and testing time: 6640.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Section 5: Train, Evaluate, and Predict Custom URL\n",
    "# ---------------------------\n",
    "\n",
    "rf = RandomForest(n_trees=5, max_depth=10, min_size=5)\n",
    "start_time = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict_all(X_test)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tp = sum((yt == 1 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    tn = sum((yt == 0 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "    fp = sum((yt == 0 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    fn = sum((yt == 1 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "acc, prec, rec, f1 = compute_metrics(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Training and testing time:\", round(time.time() - start_time, 2), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e21312-f053-4a2b-808f-f2001e0fb018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URLs one by one. Type 'exit' to stop:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  www.google.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: www.google.com/ => Prediction: Benign\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Check another? (y/n):  y\n",
      "Enter URL:  www.g00gle.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: www.g00gle.com => Prediction: Malicious\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Check another? (y/n):  y\n",
      "Enter URL:  www.csueastbay.edu/canvaslogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: www.csueastbay.edu/canvaslogin => Prediction: Benign\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Check another? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting URL checker.\n"
     ]
    }
   ],
   "source": [
    "# Custom input prediction loop with continue confirmation\n",
    "print(\"Enter URLs one by one. Type 'exit' to stop:\")\n",
    "while True:\n",
    "    url = input(\"Enter URL: \").strip()\n",
    "    if url.lower() == 'exit':\n",
    "        break\n",
    "    if url:\n",
    "        features = extract_features(url)\n",
    "        norm_features = normalize_features([features])[0]\n",
    "        prediction = rf.predict(norm_features)\n",
    "        result = \"Malicious\" if prediction == 1 else \"Benign\"\n",
    "        print(f\"URL: {url} => Prediction: {result}\")\n",
    "        cont = input(\"Check another? (y/n): \").strip().lower()\n",
    "        if cont != 'y':\n",
    "            print(\"Exiting URL checker.\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baf711-d30c-4484-b085-3c408c709eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5f48-9d67-461c-8e0d-521606171dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
